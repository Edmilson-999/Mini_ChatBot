#  Mini Chatbot de Nutri√ß√£o com RAG e Mistral-7B

Este projeto implementa um **Mini Chatbot baseado em Retrieval-Augmented Generation (RAG)** utilizando o modelo **Mistral-7B-Instruct-v0.2** e a biblioteca **LangChain**.
O chatbot responde a perguntas sobre **nutri√ß√£o**, recuperando informa√ß√µes relevantes de uma base de documentos (`.txt` e `.pdf`) e gerando respostas **precisas e contextualizadas**.

---

## Objetivo

Criar um sistema que combine:

* **Recupera√ß√£o de informa√ß√µes** com FAISS (vetor store)
* **Gera√ß√£o de respostas** com o modelo Mistral-7B

Exemplos de perguntas:

* ‚ÄúQuais alimentos s√£o ricos em ferro?‚Äù
* ‚ÄúComo planejar uma dieta equilibrada?‚Äù

O tema **nutri√ß√£o** foi escolhido pela sua relev√¢ncia social e pela ampla disponibilidade de conte√∫do confi√°vel.

---

##  Tecnologias Utilizadas

| Tecnologia               | Fun√ß√£o                                     |
| ------------------------ | ------------------------------------------ |
| Python 3.11              | Linguagem base do projeto                  |
| LangChain                | Framework para pipeline RAG                |
| Mistral-7B-Instruct-v0.2 | Modelo de linguagem local (formato GGUF)   |
| FAISS                    | Vetor store para busca sem√¢ntica           |
| Sentence Transformers    | Gera√ß√£o de embeddings (`all-MiniLM-L6-v2`) |
| llama-cpp-python         | Execu√ß√£o local do modelo Mistral           |
| PyPDF2                   | Processamento de arquivos PDF              |

---

## Pr√©-requisitos

* **CPU moderna** (m√≠n. 8 n√∫cleos) ou **GPU NVIDIA** (para acelerar o modelo)
* **Mem√≥ria**: m√≠nimo 12 GB RAM (para modelos quantizados Q4)
* **Python**: 3.8 ou superior (recomenda-se 3.11)
* **Espa√ßo em Disco**: \~5 GB para o modelo e documentos

---

##  Instala√ß√£o

### 1. Criar Ambiente Virtual

```bash
conda create -n chatbot python=3.11
conda activate chatbot
```

### 2. Instalar Depend√™ncias

```bash
pip install -U langchain langchain-community langchain-huggingface pypdf faiss-cpu sentence-transformers tf-keras llama-cpp-python
```

**Com GPU NVIDIA** (opcional):

```bash
pip install llama-cpp-python --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu122
```

> üîé Substitua `cu122` pela sua vers√£o de CUDA (`nvidia-smi`)

---

### 3. Baixar o Modelo Mistral-7B

```bash
HF_HUB_ENABLE_HF_TRANSFER=1 huggingface-cli download \
TheBloke/Mistral-7B-Instruct-v0.2-GGUF mistral-7b-instruct-v0.2.Q4_K_M.gguf \
--local-dir ./modelos --local-dir-use-symlinks False
```

---

### 4. Preparar os Documentos

```bash
mkdir -p data/nutricao
```

Adicione de **10 a 50 arquivos `.txt` ou `.pdf`** com conte√∫dos sobre nutri√ß√£o:
*guias, artigos cient√≠ficos, receitas saud√°veis, materiais da OMS, etc.*

---

##  Configura√ß√£o

### üîó Caminho do modelo

No script `chatbot_rag.py`, atualize:

```python
llm = LlamaCpp(
    model_path="./model/mistral-7b-instruct-v0.2.Q4_K_M.gguf",
    ...
)
```

###  Ativando GPU (opcional)

```python
llm = LlamaCpp(
    model_path="./modelos/mistral-7b-instruct-v0.2.Q4_K_M.gguf",
    n_ctx=2048,
    max_tokens=500,
    temperature=0.7,
    n_gpu_layers=40,
    verbose=True
)
```

---

##  Como Usar

Execute o script:

```bash
python chatbot_rag.py
```

Voc√™ ver√° no terminal:

```
 Bem-vindo ao Chatbot de Nutri√ß√£o!
Digite sua pergunta ou 'sair' para encerrar.
```

Fa√ßa perguntas como:

* ‚ÄúQuais alimentos s√£o ricos em ferro?‚Äù
* ‚ÄúComo planejar uma dieta rica em fibras?‚Äù

Para encerrar, digite `sair`.

---

##  Exemplo de Sa√≠da

```
Pergunta: Quais alimentos s√£o ricos em ferro?

Resposta: Alimentos ricos em ferro incluem carne vermelha, f√≠gado, espinafre, lentilhas, feij√£o e cereais fortificados. O ferro heme, encontrado em produtos animais, √© mais facilmente absorvido pelo corpo.

Fontes: [{'source': 'data/nutricao/guia_nutrientes.pdf', 'page': 3}, {'source': 'data/nutricao/artigo_ferro.txt'}]

Pergunta: sair
Encerrando o chatbot...
```

---

##  Estrutura do Projeto

```
mini-chatbot-rag/
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îî‚îÄ‚îÄ nutricao/                   # Arquivos .txt e .pdf
‚îú‚îÄ‚îÄ modelos/
‚îÇ   ‚îî‚îÄ‚îÄ mistral-7b-instruct-v0.2.Q4_K_M.gguf
‚îú‚îÄ‚îÄ chatbot_rag.py                 # Script principal
‚îî‚îÄ‚îÄ README.md
```

---

## Pipeline RAG

1. **Leitura dos documentos** (.txt/.pdf)
2. **Divis√£o em chunks** (\~1000 caracteres, com sobreposi√ß√£o de 200)
3. **Gera√ß√£o de embeddings** com `all-MiniLM-L6-v2`
4. **Armazenamento vetorial** no FAISS
5. **Busca sem√¢ntica** com LangChain
6. **Resposta gerada** com Mistral-7B

---

##  Performance

* **Tempo de resposta (CPU)**: 5‚Äì25 segundos
* **Tempo de resposta (GPU)**: 2‚Äì6 segundos

### Dicas de otimiza√ß√£o:

* Reduza `n_ctx` (ex: 1024) ou `max_tokens`
* Use quantiza√ß√µes menores (ex: Q2\_K)
* Use GPU se poss√≠vel

---

##  Problemas Conhecidos

* **Erro ao sair**:

  ```
  TypeError: 'NoneType' object is not callable
  ```

  Bug conhecido do `llama-cpp-python`. N√£o afeta o uso. Pode ser mitigado liberando o modelo manualmente.

* **Baixo desempenho em PCs com <12 GB RAM**: use quantiza√ß√µes mais leves ou reduza `n_ctx`.

---

##  Contribui√ß√µes

Este projeto foi desenvolvido para fins acad√™micos.

Sinta-se √† vontade para sugerir:

* Suporte a outros modelos (ex.: LLaMA, Gemma)
* Interfaces gr√°ficas com Gradio ou Streamlit
* Otimiza√ß√£o para bases maiores

---

## üìÑ Licen√ßa

Distribu√≠do sob a **licen√ßa MIT**.
